[38;5;2m[i 0509 17:40:58.826525 84 compiler.py:951] Jittor(1.3.3.14) src: /home/prs01/miniconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0509 17:40:58.844071 84 compiler.py:952] g++ at /usr/bin/g++(9.4.0)[m
[38;5;2m[i 0509 17:40:58.844229 84 compiler.py:953] cache_path: /home/prs01/.cache/jittor/jt1.3.3/g++9.4.0/py3.7.13/Linux-5.4.0-10xc4/IntelRXeonRGolx7a/default[m
[38;5;2m[i 0509 17:40:58.888472 84 install_cuda.py:53] cuda_driver_version: [11, 6][m
[38;5;2m[i 0509 17:40:58.905847 84 __init__.py:411] Found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0509 17:40:58.923234 84 __init__.py:411] Found addr2line(2.34) at /usr/bin/addr2line.[m
[38;5;2m[i 0509 17:40:59.102890 84 compiler.py:1006] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0509 17:40:59.403214 84 __init__.py:227] Total mem: 251.56GB, using 16 procs for compiling.[m
[38;5;2m[i 0509 17:40:59.526857 84 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0509 17:40:59.693714 84 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0509 17:40:59.827679 84 compile_extern.py:516] mpicc not found, distribution disabled.[m
[38;5;2m[i 0509 17:40:59.904901 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cublas.h[m
[38;5;2m[i 0509 17:40:59.932959 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcublas.so[m
[38;5;2m[i 0509 17:40:59.933237 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcublasLt.so.11[m
[38;5;2m[i 0509 17:41:00.926100 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cudnn.h[m
[38;5;2m[i 0509 17:41:00.973946 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn.so.8[m
[38;5;2m[i 0509 17:41:00.974098 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_ops_infer.so.8[m
[38;5;2m[i 0509 17:41:00.989584 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_ops_train.so.8[m
[38;5;2m[i 0509 17:41:00.998261 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_cnn_infer.so.8[m
[38;5;2m[i 0509 17:41:01.039928 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_cnn_train.so.8[m
[38;5;2m[i 0509 17:41:01.986807 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/curand.h[m
[38;5;2m[i 0509 17:41:02.028167 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcurand.so[m
[38;5;2m[i 0509 17:41:02.088227 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cufft.h[m
[38;5;2m[i 0509 17:41:02.131343 84 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcufft.so[m
[38;5;2m[i 0509 17:41:02.303205 84 cuda_flags.cc:32] CUDA enabled.[m
Loading data...
[38;5;2m[i 0509 17:41:02.804591 84 dataset.py:631] Found 102 classes and 1020 images.[m
[38;5;2m[i 0509 17:41:02.834613 84 dataset.py:631] Found 102 classes and 1020 images.[m
[38;5;2m[i 0509 17:41:02.900244 84 dataset.py:631] Found 102 classes and 6149 images.[m
----------------- A new trial ---------------------
modelName MLPMixer learning rate: 2.3e-05 etaMin 1e-05 imgSize 224 savedName 1_1.pkl criterion CrossEntropyLoss weight_decay 0.001 TMax 15
Training:   0%|          | 0/200 [00:00<?, ?it/s][38;5;3m[w 0509 17:41:03.336829 84 grad.cc:77] grads[0] 'model.0.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2003:1:2:2:i1:o1:s0:n1,float32,model.0.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.336878 84 grad.cc:77] grads[1] 'model.0.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2007:1:2:2:i1:o1:s0:n0,float32,model.0.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.336891 84 grad.cc:77] grads[2] 'model.0.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2011:1:2:2:i1:o1:s0:n1,float32,model.0.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.336897 84 grad.cc:77] grads[3] 'model.0.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2015:1:2:2:i1:o1:s0:n0,float32,model.0.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.336903 84 grad.cc:77] grads[4] 'model.0.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2019:1:2:2:i1:o1:s0:n0,float32,model.0.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.336910 84 grad.cc:77] grads[5] 'model.0.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2023:1:2:2:i1:o1:s0:n0,float32,model.0.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.336916 84 grad.cc:77] grads[6] 'model.0.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2027:1:2:2:i1:o1:s0:n1,float32,model.0.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.336922 84 grad.cc:77] grads[7] 'model.0.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2031:1:2:2:i1:o1:s0:n0,float32,model.0.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.336928 84 grad.cc:77] grads[8] 'model.0.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2035:1:2:2:i1:o1:s0:n1,float32,model.0.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.336933 84 grad.cc:77] grads[9] 'model.0.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2039:1:2:2:i1:o1:s0:n0,float32,model.0.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348019 84 grad.cc:77] grads[10] 'model.0.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2043:1:2:2:i1:o1:s0:n0,float32,model.0.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348030 84 grad.cc:77] grads[11] 'model.0.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2047:1:2:2:i1:o1:s0:n0,float32,model.0.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348037 84 grad.cc:77] grads[12] 'model.1.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2051:1:2:2:i1:o1:s0:n1,float32,model.1.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.348044 84 grad.cc:77] grads[13] 'model.1.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2055:1:2:2:i1:o1:s0:n0,float32,model.1.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.348051 84 grad.cc:77] grads[14] 'model.1.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2059:1:2:2:i1:o1:s0:n1,float32,model.1.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.348057 84 grad.cc:77] grads[15] 'model.1.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2063:1:2:2:i1:o1:s0:n0,float32,model.1.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.348064 84 grad.cc:77] grads[16] 'model.1.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2067:1:2:2:i1:o1:s0:n0,float32,model.1.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348071 84 grad.cc:77] grads[17] 'model.1.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2071:1:2:2:i1:o1:s0:n0,float32,model.1.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348078 84 grad.cc:77] grads[18] 'model.1.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2075:1:2:2:i1:o1:s0:n1,float32,model.1.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.348085 84 grad.cc:77] grads[19] 'model.1.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2079:1:2:2:i1:o1:s0:n0,float32,model.1.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.348091 84 grad.cc:77] grads[20] 'model.1.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2083:1:2:2:i1:o1:s0:n1,float32,model.1.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.348098 84 grad.cc:77] grads[21] 'model.1.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2087:1:2:2:i1:o1:s0:n0,float32,model.1.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348105 84 grad.cc:77] grads[22] 'model.1.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2091:1:2:2:i1:o1:s0:n0,float32,model.1.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348115 84 grad.cc:77] grads[23] 'model.1.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2095:1:2:2:i1:o1:s0:n0,float32,model.1.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348122 84 grad.cc:77] grads[24] 'model.2.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2099:1:2:2:i1:o1:s0:n1,float32,model.2.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.348129 84 grad.cc:77] grads[25] 'model.2.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2103:1:2:2:i1:o1:s0:n0,float32,model.2.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.348138 84 grad.cc:77] grads[26] 'model.2.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2107:1:2:2:i1:o1:s0:n1,float32,model.2.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.348148 84 grad.cc:77] grads[27] 'model.2.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2111:1:2:2:i1:o1:s0:n0,float32,model.2.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.348159 84 grad.cc:77] grads[28] 'model.2.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2115:1:2:2:i1:o1:s0:n0,float32,model.2.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348169 84 grad.cc:77] grads[29] 'model.2.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2119:1:2:2:i1:o1:s0:n0,float32,model.2.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348178 84 grad.cc:77] grads[30] 'model.2.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2123:1:2:2:i1:o1:s0:n1,float32,model.2.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.348185 84 grad.cc:77] grads[31] 'model.2.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2127:1:2:2:i1:o1:s0:n0,float32,model.2.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.348192 84 grad.cc:77] grads[32] 'model.2.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2131:1:2:2:i1:o1:s0:n1,float32,model.2.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.348199 84 grad.cc:77] grads[33] 'model.2.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2135:1:2:2:i1:o1:s0:n0,float32,model.2.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348206 84 grad.cc:77] grads[34] 'model.2.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2139:1:2:2:i1:o1:s0:n0,float32,model.2.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348213 84 grad.cc:77] grads[35] 'model.2.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2143:1:2:2:i1:o1:s0:n0,float32,model.2.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348220 84 grad.cc:77] grads[36] 'model.3.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2147:1:2:2:i1:o1:s0:n1,float32,model.3.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.348227 84 grad.cc:77] grads[37] 'model.3.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2151:1:2:2:i1:o1:s0:n0,float32,model.3.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.348233 84 grad.cc:77] grads[38] 'model.3.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2155:1:2:2:i1:o1:s0:n1,float32,model.3.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.348240 84 grad.cc:77] grads[39] 'model.3.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2159:1:2:2:i1:o1:s0:n0,float32,model.3.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.348247 84 grad.cc:77] grads[40] 'model.3.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2163:1:2:2:i1:o1:s0:n0,float32,model.3.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348253 84 grad.cc:77] grads[41] 'model.3.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2167:1:2:2:i1:o1:s0:n0,float32,model.3.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348260 84 grad.cc:77] grads[42] 'model.3.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2171:1:2:2:i1:o1:s0:n1,float32,model.3.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.348270 84 grad.cc:77] grads[43] 'model.3.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2175:1:2:2:i1:o1:s0:n0,float32,model.3.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.348277 84 grad.cc:77] grads[44] 'model.3.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2179:1:2:2:i1:o1:s0:n1,float32,model.3.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.348285 84 grad.cc:77] grads[45] 'model.3.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2183:1:2:2:i1:o1:s0:n0,float32,model.3.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348292 84 grad.cc:77] grads[46] 'model.3.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2187:1:2:2:i1:o1:s0:n0,float32,model.3.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348298 84 grad.cc:77] grads[47] 'model.3.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2191:1:2:2:i1:o1:s0:n0,float32,model.3.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348305 84 grad.cc:77] grads[48] 'model.4.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2195:1:2:2:i1:o1:s0:n1,float32,model.4.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.348312 84 grad.cc:77] grads[49] 'model.4.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2199:1:2:2:i1:o1:s0:n0,float32,model.4.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.348320 84 grad.cc:77] grads[50] 'model.4.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2203:1:2:2:i1:o1:s0:n1,float32,model.4.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.348326 84 grad.cc:77] grads[51] 'model.4.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2207:1:2:2:i1:o1:s0:n0,float32,model.4.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.348334 84 grad.cc:77] grads[52] 'model.4.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2211:1:2:2:i1:o1:s0:n0,float32,model.4.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348341 84 grad.cc:77] grads[53] 'model.4.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2215:1:2:2:i1:o1:s0:n0,float32,model.4.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348348 84 grad.cc:77] grads[54] 'model.4.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2219:1:2:2:i1:o1:s0:n1,float32,model.4.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.348355 84 grad.cc:77] grads[55] 'model.4.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2223:1:2:2:i1:o1:s0:n0,float32,model.4.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.348361 84 grad.cc:77] grads[56] 'model.4.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2227:1:2:2:i1:o1:s0:n1,float32,model.4.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.348368 84 grad.cc:77] grads[57] 'model.4.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2231:1:2:2:i1:o1:s0:n0,float32,model.4.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348375 84 grad.cc:77] grads[58] 'model.4.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2235:1:2:2:i1:o1:s0:n0,float32,model.4.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348382 84 grad.cc:77] grads[59] 'model.4.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2239:1:2:2:i1:o1:s0:n0,float32,model.4.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348388 84 grad.cc:77] grads[60] 'model.5.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2243:1:2:2:i1:o1:s0:n1,float32,model.5.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.348398 84 grad.cc:77] grads[61] 'model.5.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2247:1:2:2:i1:o1:s0:n0,float32,model.5.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.348410 84 grad.cc:77] grads[62] 'model.5.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2251:1:2:2:i1:o1:s0:n1,float32,model.5.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.348423 84 grad.cc:77] grads[63] 'model.5.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2255:1:2:2:i1:o1:s0:n0,float32,model.5.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.348434 84 grad.cc:77] grads[64] 'model.5.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2259:1:2:2:i1:o1:s0:n0,float32,model.5.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348932 84 grad.cc:77] grads[65] 'model.5.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2263:1:2:2:i1:o1:s0:n0,float32,model.5.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348942 84 grad.cc:77] grads[66] 'model.5.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2267:1:2:2:i1:o1:s0:n1,float32,model.5.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.348952 84 grad.cc:77] grads[67] 'model.5.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2271:1:2:2:i1:o1:s0:n0,float32,model.5.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.348960 84 grad.cc:77] grads[68] 'model.5.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2275:1:2:2:i1:o1:s0:n1,float32,model.5.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.348969 84 grad.cc:77] grads[69] 'model.5.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2279:1:2:2:i1:o1:s0:n0,float32,model.5.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348977 84 grad.cc:77] grads[70] 'model.5.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2283:1:2:2:i1:o1:s0:n0,float32,model.5.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.348985 84 grad.cc:77] grads[71] 'model.5.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2287:1:2:2:i1:o1:s0:n0,float32,model.5.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.348993 84 grad.cc:77] grads[72] 'model.6.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2291:1:2:2:i1:o1:s0:n1,float32,model.6.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.349001 84 grad.cc:77] grads[73] 'model.6.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2295:1:2:2:i1:o1:s0:n0,float32,model.6.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.349010 84 grad.cc:77] grads[74] 'model.6.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2299:1:2:2:i1:o1:s0:n1,float32,model.6.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.349018 84 grad.cc:77] grads[75] 'model.6.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2303:1:2:2:i1:o1:s0:n0,float32,model.6.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.349027 84 grad.cc:77] grads[76] 'model.6.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2307:1:2:2:i1:o1:s0:n0,float32,model.6.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349037 84 grad.cc:77] grads[77] 'model.6.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2311:1:2:2:i1:o1:s0:n0,float32,model.6.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349049 84 grad.cc:77] grads[78] 'model.6.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2315:1:2:2:i1:o1:s0:n1,float32,model.6.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.349062 84 grad.cc:77] grads[79] 'model.6.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2319:1:2:2:i1:o1:s0:n0,float32,model.6.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.349074 84 grad.cc:77] grads[80] 'model.6.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2323:1:2:2:i1:o1:s0:n1,float32,model.6.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.349087 84 grad.cc:77] grads[81] 'model.6.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2327:1:2:2:i1:o1:s0:n0,float32,model.6.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349096 84 grad.cc:77] grads[82] 'model.6.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2331:1:2:2:i1:o1:s0:n0,float32,model.6.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349107 84 grad.cc:77] grads[83] 'model.6.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2335:1:2:2:i1:o1:s0:n0,float32,model.6.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349115 84 grad.cc:77] grads[84] 'model.7.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2339:1:2:2:i1:o1:s0:n1,float32,model.7.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.349122 84 grad.cc:77] grads[85] 'model.7.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2343:1:2:2:i1:o1:s0:n0,float32,model.7.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.349130 84 grad.cc:77] grads[86] 'model.7.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2347:1:2:2:i1:o1:s0:n1,float32,model.7.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.349139 84 grad.cc:77] grads[87] 'model.7.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2351:1:2:2:i1:o1:s0:n0,float32,model.7.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.349147 84 grad.cc:77] grads[88] 'model.7.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2355:1:2:2:i1:o1:s0:n0,float32,model.7.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349155 84 grad.cc:77] grads[89] 'model.7.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2359:1:2:2:i1:o1:s0:n0,float32,model.7.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349163 84 grad.cc:77] grads[90] 'model.7.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2363:1:2:2:i1:o1:s0:n1,float32,model.7.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.349172 84 grad.cc:77] grads[91] 'model.7.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2367:1:2:2:i1:o1:s0:n0,float32,model.7.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.349179 84 grad.cc:77] grads[92] 'model.7.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2371:1:2:2:i1:o1:s0:n1,float32,model.7.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.349188 84 grad.cc:77] grads[93] 'model.7.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2375:1:2:2:i1:o1:s0:n0,float32,model.7.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349196 84 grad.cc:77] grads[94] 'model.7.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2379:1:2:2:i1:o1:s0:n0,float32,model.7.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349204 84 grad.cc:77] grads[95] 'model.7.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2383:1:2:2:i1:o1:s0:n0,float32,model.7.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349211 84 grad.cc:77] grads[96] 'model.8.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2387:1:2:2:i1:o1:s0:n1,float32,model.8.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.349220 84 grad.cc:77] grads[97] 'model.8.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2391:1:2:2:i1:o1:s0:n0,float32,model.8.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.349229 84 grad.cc:77] grads[98] 'model.8.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2395:1:2:2:i1:o1:s0:n1,float32,model.8.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.349237 84 grad.cc:77] grads[99] 'model.8.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2399:1:2:2:i1:o1:s0:n0,float32,model.8.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.349246 84 grad.cc:77] grads[100] 'model.8.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2403:1:2:2:i1:o1:s0:n0,float32,model.8.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349254 84 grad.cc:77] grads[101] 'model.8.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2407:1:2:2:i1:o1:s0:n0,float32,model.8.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349262 84 grad.cc:77] grads[102] 'model.8.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2411:1:2:2:i1:o1:s0:n1,float32,model.8.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.349275 84 grad.cc:77] grads[103] 'model.8.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2415:1:2:2:i1:o1:s0:n0,float32,model.8.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.349283 84 grad.cc:77] grads[104] 'model.8.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2419:1:2:2:i1:o1:s0:n1,float32,model.8.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.349859 84 grad.cc:77] grads[105] 'model.8.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2423:1:2:2:i1:o1:s0:n0,float32,model.8.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349870 84 grad.cc:77] grads[106] 'model.8.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2427:1:2:2:i1:o1:s0:n0,float32,model.8.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349878 84 grad.cc:77] grads[107] 'model.8.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2431:1:2:2:i1:o1:s0:n0,float32,model.8.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349886 84 grad.cc:77] grads[108] 'model.9.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2435:1:2:2:i1:o1:s0:n1,float32,model.9.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.349894 84 grad.cc:77] grads[109] 'model.9.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2439:1:2:2:i1:o1:s0:n0,float32,model.9.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.349903 84 grad.cc:77] grads[110] 'model.9.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2443:1:2:2:i1:o1:s0:n1,float32,model.9.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.349911 84 grad.cc:77] grads[111] 'model.9.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2447:1:2:2:i1:o1:s0:n0,float32,model.9.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.349919 84 grad.cc:77] grads[112] 'model.9.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2451:1:2:2:i1:o1:s0:n0,float32,model.9.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349928 84 grad.cc:77] grads[113] 'model.9.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2455:1:2:2:i1:o1:s0:n0,float32,model.9.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349937 84 grad.cc:77] grads[114] 'model.9.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2459:1:2:2:i1:o1:s0:n1,float32,model.9.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.349945 84 grad.cc:77] grads[115] 'model.9.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2463:1:2:2:i1:o1:s0:n0,float32,model.9.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.349954 84 grad.cc:77] grads[116] 'model.9.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2467:1:2:2:i1:o1:s0:n1,float32,model.9.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.349963 84 grad.cc:77] grads[117] 'model.9.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2471:1:2:2:i1:o1:s0:n0,float32,model.9.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349971 84 grad.cc:77] grads[118] 'model.9.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2475:1:2:2:i1:o1:s0:n0,float32,model.9.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.349979 84 grad.cc:77] grads[119] 'model.9.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2479:1:2:2:i1:o1:s0:n0,float32,model.9.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.349988 84 grad.cc:77] grads[120] 'model.10.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2483:1:2:2:i1:o1:s0:n1,float32,model.10.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.349997 84 grad.cc:77] grads[121] 'model.10.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2487:1:2:2:i1:o1:s0:n0,float32,model.10.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.350006 84 grad.cc:77] grads[122] 'model.10.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2491:1:2:2:i1:o1:s0:n1,float32,model.10.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.350018 84 grad.cc:77] grads[123] 'model.10.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2495:1:2:2:i1:o1:s0:n0,float32,model.10.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.350030 84 grad.cc:77] grads[124] 'model.10.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2499:1:2:2:i1:o1:s0:n0,float32,model.10.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.350042 84 grad.cc:77] grads[125] 'model.10.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2503:1:2:2:i1:o1:s0:n0,float32,model.10.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.350058 84 grad.cc:77] grads[126] 'model.10.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2507:1:2:2:i1:o1:s0:n1,float32,model.10.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.350070 84 grad.cc:77] grads[127] 'model.10.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2511:1:2:2:i1:o1:s0:n0,float32,model.10.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.350084 84 grad.cc:77] grads[128] 'model.10.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2515:1:2:2:i1:o1:s0:n1,float32,model.10.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.350096 84 grad.cc:77] grads[129] 'model.10.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2519:1:2:2:i1:o1:s0:n0,float32,model.10.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.350104 84 grad.cc:77] grads[130] 'model.10.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2523:1:2:2:i1:o1:s0:n0,float32,model.10.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.350112 84 grad.cc:77] grads[131] 'model.10.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2527:1:2:2:i1:o1:s0:n0,float32,model.10.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.350120 84 grad.cc:77] grads[132] 'model.11.0.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2531:1:2:2:i1:o1:s0:n1,float32,model.11.0.fn.net.0.bias,0)[784,][m
[38;5;3m[w 0509 17:41:03.403416 84 grad.cc:77] grads[133] 'model.11.0.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2535:1:2:2:i1:o1:s0:n0,float32,model.11.0.fn.net.0.weight,0)[784,196,1,][m
[38;5;3m[w 0509 17:41:03.403433 84 grad.cc:77] grads[134] 'model.11.0.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2539:1:2:2:i1:o1:s0:n1,float32,model.11.0.fn.net.3.bias,0)[196,][m
[38;5;3m[w 0509 17:41:03.403442 84 grad.cc:77] grads[135] 'model.11.0.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2543:1:2:2:i1:o1:s0:n0,float32,model.11.0.fn.net.3.weight,0)[196,784,1,][m
[38;5;3m[w 0509 17:41:03.403451 84 grad.cc:77] grads[136] 'model.11.0.norm.weight' doesn't have gradient. It will be set to zero: Var(2547:1:2:2:i1:o1:s0:n0,float32,model.11.0.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.403459 84 grad.cc:77] grads[137] 'model.11.0.norm.bias' doesn't have gradient. It will be set to zero: Var(2551:1:2:2:i1:o1:s0:n0,float32,model.11.0.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.403467 84 grad.cc:77] grads[138] 'model.11.1.fn.net.0.weight' doesn't have gradient. It will be set to zero: Var(2555:1:2:2:i1:o1:s0:n1,float32,model.11.1.fn.net.0.weight,0)[2048,512,][m
[38;5;3m[w 0509 17:41:03.403476 84 grad.cc:77] grads[139] 'model.11.1.fn.net.0.bias' doesn't have gradient. It will be set to zero: Var(2559:1:2:2:i1:o1:s0:n0,float32,model.11.1.fn.net.0.bias,0)[2048,][m
[38;5;3m[w 0509 17:41:03.403484 84 grad.cc:77] grads[140] 'model.11.1.fn.net.3.weight' doesn't have gradient. It will be set to zero: Var(2563:1:2:2:i1:o1:s0:n1,float32,model.11.1.fn.net.3.weight,0)[512,2048,][m
[38;5;3m[w 0509 17:41:03.403492 84 grad.cc:77] grads[141] 'model.11.1.fn.net.3.bias' doesn't have gradient. It will be set to zero: Var(2567:1:2:2:i1:o1:s0:n0,float32,model.11.1.fn.net.3.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.403500 84 grad.cc:77] grads[142] 'model.11.1.norm.weight' doesn't have gradient. It will be set to zero: Var(2571:1:2:2:i1:o1:s0:n0,float32,model.11.1.norm.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.403511 84 grad.cc:77] grads[143] 'model.11.1.norm.bias' doesn't have gradient. It will be set to zero: Var(2575:1:2:2:i1:o1:s0:n0,float32,model.11.1.norm.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.403520 84 grad.cc:77] grads[144] 'patcher.0.weight' doesn't have gradient. It will be set to zero: Var(2579:1:2:2:i1:o1:s0:n1,float32,patcher.0.weight,0)[512,3,16,16,][m
[38;5;3m[w 0509 17:41:03.403531 84 grad.cc:77] grads[145] 'patcher.0.bias' doesn't have gradient. It will be set to zero: Var(2583:1:2:2:i1:o1:s0:n1,float32,patcher.0.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.403539 84 grad.cc:77] grads[146] 'active.weight' doesn't have gradient. It will be set to zero: Var(2587:1:2:2:i1:o1:s0:n0,float32,active.weight,0)[512,][m
[38;5;3m[w 0509 17:41:03.403549 84 grad.cc:77] grads[147] 'active.bias' doesn't have gradient. It will be set to zero: Var(2591:1:2:2:i1:o1:s0:n0,float32,active.bias,0)[512,][m
[38;5;3m[w 0509 17:41:03.403556 84 grad.cc:77] grads[148] 'mlp_head.0.weight' doesn't have gradient. It will be set to zero: Var(2595:1:2:2:i1:o1:s0:n1,float32,mlp_head.0.weight,0)[5001,512,][m
[38;5;3m[w 0509 17:41:03.403565 84 grad.cc:77] grads[149] 'mlp_head.0.bias' doesn't have gradient. It will be set to zero: Var(2599:1:2:2:i1:o1:s0:n0,float32,mlp_head.0.bias,0)[5001,][m
epoch: 0 validateAccuracy: 0.0225 trainAccuracy: 0.0049 delta: 0.0225
Training:   0%|          | 1/200 [00:21<1:09:42, 21.02s/it]epoch: 1 validateAccuracy: 0.0559 trainAccuracy: 0.0412 delta: 0.0334
Training:   1%|          | 2/200 [00:40<1:05:26, 19.83s/it]epoch: 2 validateAccuracy: 0.102 trainAccuracy: 0.1402 delta: 0.0461
Training:   2%|â–         | 3/200 [00:59<1:04:52, 19.76s/it]epoch: 3 validateAccuracy: 0.1412 trainAccuracy: 0.2627 delta: 0.0392
Training:   2%|â–         | 4/200 [01:19<1:04:30, 19.75s/it]epoch: 4 validateAccuracy: 0.1745 trainAccuracy: 0.3765 delta: 0.0333
Training:   2%|â–Ž         | 5/200 [01:39<1:04:01, 19.70s/it]epoch: 5 validateAccuracy: 0.202 trainAccuracy: 0.5098 delta: 0.0275
Training:   3%|â–Ž         | 6/200 [01:58<1:03:39, 19.69s/it]epoch: 6 validateAccuracy: 0.2196 trainAccuracy: 0.6333 delta: 0.0176
Training:   4%|â–Ž         | 7/200 [02:18<1:02:58, 19.58s/it]epoch: 7 validateAccuracy: 0.2343 trainAccuracy: 0.7157 delta: 0.0147
Training:   4%|â–         | 8/200 [02:37<1:02:10, 19.43s/it]epoch: 8 validateAccuracy: 0.2333 trainAccuracy: 0.7961 delta: -0.001
Training:   4%|â–         | 9/200 [02:54<1:00:13, 18.92s/it]epoch: 9 validateAccuracy: 0.2324 trainAccuracy: 0.8598 delta: -0.0019
Training:   5%|â–Œ         | 10/200 [03:12<58:19, 18.42s/it] epoch: 10 validateAccuracy: 0.2451 trainAccuracy: 0.9098 delta: 0.0108
Training:   6%|â–Œ         | 11/200 [03:31<59:14, 18.81s/it]epoch: 11 validateAccuracy: 0.2382 trainAccuracy: 0.951 delta: -0.0069
Training:   6%|â–Œ         | 12/200 [03:49<58:04, 18.53s/it]epoch: 12 validateAccuracy: 0.2461 trainAccuracy: 0.9657 delta: 0.001
Training:   6%|â–‹         | 13/200 [04:09<58:47, 18.87s/it]epoch: 13 validateAccuracy: 0.2451 trainAccuracy: 0.9804 delta: -0.001
Training:   7%|â–‹         | 14/200 [04:27<57:23, 18.51s/it]epoch: 14 validateAccuracy: 0.2412 trainAccuracy: 0.9804 delta: -0.0049
Training:   8%|â–Š         | 15/200 [04:44<56:00, 18.16s/it]epoch: 15 validateAccuracy: 0.249 trainAccuracy: 0.9824 delta: 0.0029
Training:   8%|â–Š         | 16/200 [05:04<57:13, 18.66s/it]epoch: 16 validateAccuracy: 0.2461 trainAccuracy: 0.9922 delta: -0.0029
Training:   8%|â–Š         | 17/200 [05:22<56:02, 18.37s/it]epoch: 17 validateAccuracy: 0.25 trainAccuracy: 0.9941 delta: 0.001
Training:   9%|â–‰         | 18/200 [05:41<56:52, 18.75s/it]epoch: 18 validateAccuracy: 0.2422 trainAccuracy: 0.9941 delta: -0.0078
Training:  10%|â–‰         | 19/200 [05:59<55:42, 18.47s/it]epoch: 19 validateAccuracy: 0.248 trainAccuracy: 0.9971 delta: -0.002
Training:  10%|â–ˆ         | 20/200 [06:17<54:47, 18.26s/it]epoch: 20 validateAccuracy: 0.2461 trainAccuracy: 0.9971 delta: -0.0039
Training:  10%|â–ˆ         | 21/200 [06:35<54:14, 18.18s/it]epoch: 21 validateAccuracy: 0.2588 trainAccuracy: 0.999 delta: 0.0088
Training:  11%|â–ˆ         | 22/200 [06:55<55:42, 18.78s/it]epoch: 22 validateAccuracy: 0.2539 trainAccuracy: 1.0 delta: -0.0049
Training:  12%|â–ˆâ–        | 23/200 [07:13<54:42, 18.55s/it]epoch: 23 validateAccuracy: 0.249 trainAccuracy: 0.999 delta: -0.0098
Training:  12%|â–ˆâ–        | 24/200 [07:30<53:19, 18.18s/it]epoch: 24 validateAccuracy: 0.248 trainAccuracy: 0.998 delta: -0.0108
Training:  12%|â–ˆâ–Ž        | 25/200 [07:47<51:51, 17.78s/it]epoch: 25 validateAccuracy: 0.2549 trainAccuracy: 0.999 delta: -0.0039
Training:  13%|â–ˆâ–Ž        | 26/200 [08:05<51:12, 17.66s/it]epoch: 26 validateAccuracy: 0.2569 trainAccuracy: 1.0 delta: -0.0019
Training:  14%|â–ˆâ–Ž        | 27/200 [08:22<50:59, 17.69s/it]epoch: 27 validateAccuracy: 0.25 trainAccuracy: 0.999 delta: -0.0088
Training:  14%|â–ˆâ–        | 28/200 [08:40<50:40, 17.68s/it]epoch: 28 validateAccuracy: 0.25 trainAccuracy: 0.9971 delta: -0.0088
Training:  14%|â–ˆâ–        | 29/200 [08:57<50:10, 17.60s/it]epoch: 29 validateAccuracy: 0.2559 trainAccuracy: 0.999 delta: -0.0029
Training:  15%|â–ˆâ–Œ        | 30/200 [09:15<49:52, 17.60s/it]epoch: 30 validateAccuracy: 0.2461 trainAccuracy: 1.0 delta: -0.0127
Training:  16%|â–ˆâ–Œ        | 31/200 [09:32<49:12, 17.47s/it]epoch: 31 validateAccuracy: 0.2549 trainAccuracy: 0.999 delta: -0.0039
Training:  16%|â–ˆâ–Œ        | 32/200 [09:50<49:04, 17.53s/it]epoch: 32 validateAccuracy: 0.2598 trainAccuracy: 1.0 delta: 0.001
Training:  16%|â–ˆâ–‹        | 33/200 [10:09<50:28, 18.13s/it]epoch: 33 validateAccuracy: 0.2588 trainAccuracy: 0.9971 delta: -0.001
Training:  17%|â–ˆâ–‹        | 34/200 [10:27<49:57, 18.06s/it]epoch: 34 validateAccuracy: 0.2627 trainAccuracy: 0.999 delta: 0.0029
Training:  18%|â–ˆâ–Š        | 35/200 [10:47<51:14, 18.63s/it]epoch: 35 validateAccuracy: 0.2559 trainAccuracy: 1.0 delta: -0.0068
Training:  18%|â–ˆâ–Š        | 36/200 [11:05<50:24, 18.44s/it]epoch: 36 validateAccuracy: 0.2598 trainAccuracy: 1.0 delta: -0.0029
Training:  18%|â–ˆâ–Š        | 37/200 [11:23<49:17, 18.14s/it]epoch: 37 validateAccuracy: 0.2637 trainAccuracy: 0.998 delta: 0.001
Training:  19%|â–ˆâ–‰        | 38/200 [11:42<49:53, 18.48s/it]epoch: 38 validateAccuracy: 0.2676 trainAccuracy: 1.0 delta: 0.0039
Training:  20%|â–ˆâ–‰        | 39/200 [12:01<50:22, 18.77s/it]epoch: 39 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: 0.003
Training:  20%|â–ˆâ–ˆ        | 40/200 [12:21<50:36, 18.98s/it]epoch: 40 validateAccuracy: 0.2637 trainAccuracy: 1.0 delta: -0.0069
Training:  20%|â–ˆâ–ˆ        | 41/200 [12:38<49:10, 18.56s/it]epoch: 41 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.002
Training:  21%|â–ˆâ–ˆ        | 42/200 [12:56<47:59, 18.22s/it]epoch: 42 validateAccuracy: 0.2657 trainAccuracy: 1.0 delta: -0.0049
Training:  22%|â–ˆâ–ˆâ–       | 43/200 [13:13<46:48, 17.89s/it]epoch: 43 validateAccuracy: 0.2627 trainAccuracy: 1.0 delta: -0.0079
Training:  22%|â–ˆâ–ˆâ–       | 44/200 [13:31<46:36, 17.92s/it]epoch: 44 validateAccuracy: 0.2637 trainAccuracy: 1.0 delta: -0.0069
Training:  22%|â–ˆâ–ˆâ–Ž       | 45/200 [13:49<46:06, 17.85s/it]epoch: 45 validateAccuracy: 0.2647 trainAccuracy: 1.0 delta: -0.0059
Training:  23%|â–ˆâ–ˆâ–Ž       | 46/200 [14:06<45:31, 17.74s/it]epoch: 46 validateAccuracy: 0.2618 trainAccuracy: 1.0 delta: -0.0088
Training:  24%|â–ˆâ–ˆâ–Ž       | 47/200 [14:23<44:52, 17.60s/it]epoch: 47 validateAccuracy: 0.2618 trainAccuracy: 1.0 delta: -0.0088
Training:  24%|â–ˆâ–ˆâ–       | 48/200 [14:41<44:15, 17.47s/it]epoch: 48 validateAccuracy: 0.2618 trainAccuracy: 1.0 delta: -0.0088
Training:  24%|â–ˆâ–ˆâ–       | 49/200 [14:58<44:00, 17.49s/it]epoch: 49 validateAccuracy: 0.2608 trainAccuracy: 1.0 delta: -0.0098
Training:  25%|â–ˆâ–ˆâ–Œ       | 50/200 [15:16<44:14, 17.70s/it]epoch: 50 validateAccuracy: 0.2618 trainAccuracy: 1.0 delta: -0.0088
Training:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [15:34<44:03, 17.74s/it]epoch: 51 validateAccuracy: 0.2618 trainAccuracy: 1.0 delta: -0.0088
Training:  26%|â–ˆâ–ˆâ–Œ       | 52/200 [15:52<43:36, 17.68s/it]epoch: 52 validateAccuracy: 0.2608 trainAccuracy: 1.0 delta: -0.0098
Training:  26%|â–ˆâ–ˆâ–‹       | 53/200 [16:09<43:16, 17.66s/it]epoch: 53 validateAccuracy: 0.2588 trainAccuracy: 1.0 delta: -0.0118
Training:  27%|â–ˆâ–ˆâ–‹       | 54/200 [16:27<42:56, 17.65s/it]epoch: 54 validateAccuracy: 0.2608 trainAccuracy: 1.0 delta: -0.0098
Training:  28%|â–ˆâ–ˆâ–Š       | 55/200 [16:45<42:51, 17.73s/it]epoch: 55 validateAccuracy: 0.2608 trainAccuracy: 1.0 delta: -0.0098
Training:  28%|â–ˆâ–ˆâ–Š       | 56/200 [17:03<42:39, 17.78s/it]epoch: 56 validateAccuracy: 0.2637 trainAccuracy: 1.0 delta: -0.0069
Training:  28%|â–ˆâ–ˆâ–Š       | 57/200 [17:21<42:27, 17.82s/it]epoch: 57 validateAccuracy: 0.2627 trainAccuracy: 1.0 delta: -0.0079
Training:  29%|â–ˆâ–ˆâ–‰       | 58/200 [17:38<42:03, 17.77s/it]epoch: 58 validateAccuracy: 0.2627 trainAccuracy: 1.0 delta: -0.0079
Training:  30%|â–ˆâ–ˆâ–‰       | 59/200 [17:56<41:31, 17.67s/it]epoch: 59 validateAccuracy: 0.2637 trainAccuracy: 1.0 delta: -0.0069
Training:  30%|â–ˆâ–ˆâ–ˆ       | 60/200 [18:14<41:22, 17.73s/it]epoch: 60 validateAccuracy: 0.2627 trainAccuracy: 1.0 delta: -0.0079
Training:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [18:31<40:59, 17.69s/it]epoch: 61 validateAccuracy: 0.2647 trainAccuracy: 1.0 delta: -0.0059
Training:  31%|â–ˆâ–ˆâ–ˆ       | 62/200 [18:49<40:33, 17.64s/it]epoch: 62 validateAccuracy: 0.2588 trainAccuracy: 1.0 delta: -0.0118
Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [19:06<40:05, 17.55s/it]epoch: 63 validateAccuracy: 0.2529 trainAccuracy: 0.998 delta: -0.0177
Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [19:24<40:04, 17.68s/it]epoch: 64 validateAccuracy: 0.2549 trainAccuracy: 0.999 delta: -0.0157
Training:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [19:42<39:52, 17.72s/it]epoch: 65 validateAccuracy: 0.2676 trainAccuracy: 1.0 delta: -0.003
Training:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [19:59<39:17, 17.59s/it]epoch: 66 validateAccuracy: 0.2696 trainAccuracy: 1.0 delta: -0.001
Training:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [20:17<39:14, 17.70s/it]epoch: 67 validateAccuracy: 0.2657 trainAccuracy: 1.0 delta: -0.0049
Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [20:34<38:37, 17.56s/it]epoch: 68 validateAccuracy: 0.2657 trainAccuracy: 1.0 delta: -0.0049
Training:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [20:52<38:27, 17.61s/it]epoch: 69 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: 0.0
Training:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [21:09<38:00, 17.55s/it]epoch: 70 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: 0.0029
Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [21:29<39:08, 18.21s/it]epoch: 71 validateAccuracy: 0.2696 trainAccuracy: 1.0 delta: -0.0039
Training:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [21:47<38:45, 18.17s/it]epoch: 72 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0019
Training:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [22:04<37:44, 17.83s/it]epoch: 73 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0029
Training:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [22:22<37:25, 17.82s/it]epoch: 74 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [22:40<37:07, 17.82s/it]epoch: 75 validateAccuracy: 0.2657 trainAccuracy: 1.0 delta: -0.0078
Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [22:57<36:37, 17.72s/it]epoch: 76 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0019
Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [23:15<36:21, 17.74s/it]epoch: 77 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0029
Training:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [23:32<35:42, 17.56s/it]epoch: 78 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [23:50<35:18, 17.51s/it]epoch: 79 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0029
Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [24:08<35:12, 17.60s/it]epoch: 80 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0019
Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [24:25<34:53, 17.60s/it]epoch: 81 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [24:43<34:43, 17.66s/it]epoch: 82 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [25:01<34:31, 17.70s/it]epoch: 83 validateAccuracy: 0.2676 trainAccuracy: 1.0 delta: -0.0059
Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [25:18<34:12, 17.69s/it]epoch: 84 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [25:36<33:43, 17.60s/it]epoch: 85 validateAccuracy: 0.2637 trainAccuracy: 1.0 delta: -0.0098
Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [25:53<33:17, 17.53s/it]epoch: 86 validateAccuracy: 0.2647 trainAccuracy: 1.0 delta: -0.0088
Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [26:11<33:10, 17.61s/it]epoch: 87 validateAccuracy: 0.2647 trainAccuracy: 1.0 delta: -0.0088
Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [26:29<32:59, 17.68s/it]epoch: 88 validateAccuracy: 0.2627 trainAccuracy: 1.0 delta: -0.0108
Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [26:47<32:46, 17.72s/it]epoch: 89 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0029
Training:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [27:04<32:22, 17.66s/it]epoch: 90 validateAccuracy: 0.2676 trainAccuracy: 1.0 delta: -0.0059
Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [27:21<31:38, 17.42s/it]epoch: 91 validateAccuracy: 0.2676 trainAccuracy: 1.0 delta: -0.0059
Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [27:38<31:15, 17.36s/it]epoch: 92 validateAccuracy: 0.2637 trainAccuracy: 1.0 delta: -0.0098
Training:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [27:56<31:04, 17.42s/it]epoch: 93 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [28:14<31:02, 17.57s/it]epoch: 94 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [28:32<30:55, 17.67s/it]epoch: 95 validateAccuracy: 0.2676 trainAccuracy: 1.0 delta: -0.0059
Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [28:49<30:44, 17.74s/it]epoch: 96 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0049
Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [29:07<30:13, 17.60s/it]epoch: 97 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: 0.001
Training:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [29:26<30:52, 18.16s/it]epoch: 98 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: 0.001
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [29:46<31:24, 18.65s/it]epoch: 99 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [30:04<30:35, 18.36s/it]epoch: 100 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: -0.001
Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [30:21<30:00, 18.19s/it]epoch: 101 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [30:39<29:27, 18.04s/it]epoch: 102 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [30:57<28:59, 17.94s/it]epoch: 103 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [31:15<28:37, 17.89s/it]epoch: 104 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [31:32<28:08, 17.77s/it]epoch: 105 validateAccuracy: 0.2696 trainAccuracy: 1.0 delta: -0.0059
Training:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [31:50<27:42, 17.68s/it]epoch: 106 validateAccuracy: 0.2725 trainAccuracy: 1.0 delta: -0.003
Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [32:07<27:15, 17.59s/it]epoch: 107 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [32:24<26:49, 17.50s/it]epoch: 108 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [32:42<26:30, 17.47s/it]epoch: 109 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [33:00<26:31, 17.68s/it]epoch: 110 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: -0.001
Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [33:17<26:11, 17.66s/it]epoch: 111 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.002
Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [33:35<26:01, 17.74s/it]epoch: 112 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [33:53<25:40, 17.71s/it]epoch: 113 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.002
Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [34:10<25:12, 17.59s/it]epoch: 114 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.002
Training:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [34:28<24:45, 17.48s/it]epoch: 115 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.002
Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [34:45<24:29, 17.50s/it]epoch: 116 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [35:03<24:27, 17.68s/it]epoch: 117 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [35:21<24:11, 17.70s/it]epoch: 118 validateAccuracy: 0.2725 trainAccuracy: 1.0 delta: -0.003
Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [35:39<24:02, 17.81s/it]epoch: 119 validateAccuracy: 0.2696 trainAccuracy: 1.0 delta: -0.0059
Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [35:56<23:33, 17.67s/it]epoch: 120 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [36:14<23:05, 17.54s/it]epoch: 121 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.002
Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [36:31<22:51, 17.59s/it]epoch: 122 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0039
Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [36:49<22:39, 17.66s/it]epoch: 123 validateAccuracy: 0.2706 trainAccuracy: 1.0 delta: -0.0049
Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [37:07<22:28, 17.75s/it]epoch: 124 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: 0.002
Training:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [37:27<23:02, 18.43s/it]epoch: 125 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0089
Training:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [37:45<22:26, 18.19s/it]epoch: 126 validateAccuracy: 0.2686 trainAccuracy: 1.0 delta: -0.0089
Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [38:03<22:01, 18.10s/it]epoch: 127 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0059
Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [38:20<21:29, 17.91s/it]epoch: 128 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.002
Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [38:38<21:02, 17.78s/it]epoch: 129 validateAccuracy: 0.2725 trainAccuracy: 1.0 delta: -0.005
Training:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [38:55<20:33, 17.62s/it]epoch: 130 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.002
Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [39:12<20:16, 17.63s/it]epoch: 131 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0059
Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [39:30<20:01, 17.67s/it]epoch: 132 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.001
Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [39:48<19:43, 17.66s/it]epoch: 133 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.004
Training:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [40:06<19:25, 17.66s/it]epoch: 134 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0059
Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [40:23<19:09, 17.69s/it]epoch: 135 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: -0.003
Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [40:41<18:43, 17.56s/it]epoch: 136 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.002
Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [40:58<18:21, 17.49s/it]epoch: 137 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.002
Training:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [41:16<18:14, 17.65s/it]epoch: 138 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.001
Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [41:34<17:59, 17.70s/it]epoch: 139 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.001
Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [41:51<17:42, 17.71s/it]epoch: 140 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.002
Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [42:09<17:27, 17.76s/it]epoch: 141 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.001
Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [42:27<17:03, 17.65s/it]epoch: 142 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.001
Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [42:44<16:39, 17.54s/it]epoch: 143 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: 0.0009
Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [43:03<16:48, 18.00s/it]epoch: 144 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.0049
Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [43:21<16:23, 17.89s/it]epoch: 145 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0029
Training:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [43:39<16:06, 17.89s/it]epoch: 146 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.0019
Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [43:57<15:49, 17.91s/it]epoch: 147 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0009
Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [44:14<15:31, 17.91s/it]epoch: 148 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.0019
Training:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [44:30<14:44, 17.33s/it]epoch: 149 validateAccuracy: 0.2814 trainAccuracy: 1.0 delta: 0.003
Training:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [44:48<14:26, 17.34s/it]epoch: 150 validateAccuracy: 0.2814 trainAccuracy: 1.0 delta: 0.0
Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [45:03<13:38, 16.70s/it]epoch: 151 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.003
Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [45:18<12:57, 16.21s/it]epoch: 152 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.0049
Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [45:33<12:25, 15.85s/it]epoch: 153 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.0049
Training:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [45:48<12:00, 15.67s/it]epoch: 154 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0059
Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [46:04<11:38, 15.53s/it]epoch: 155 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0039
Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [46:19<11:18, 15.42s/it]epoch: 156 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.003
Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [46:34<10:59, 15.34s/it]epoch: 157 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0059
Training:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [46:49<10:41, 15.28s/it]epoch: 158 validateAccuracy: 0.2794 trainAccuracy: 1.0 delta: -0.002
Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [47:04<10:23, 15.22s/it]epoch: 159 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.003
Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [47:19<10:05, 15.14s/it]epoch: 160 validateAccuracy: 0.2804 trainAccuracy: 1.0 delta: -0.001
Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [47:34<09:50, 15.14s/it]epoch: 161 validateAccuracy: 0.2804 trainAccuracy: 1.0 delta: -0.001
Training:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [47:49<09:34, 15.11s/it]epoch: 162 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.003
Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [48:04<09:19, 15.13s/it]epoch: 163 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.003
Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [48:19<09:04, 15.12s/it]epoch: 164 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0059
Training:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [48:35<08:49, 15.13s/it]epoch: 165 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.0049
Training:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [48:50<08:33, 15.09s/it]epoch: 166 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0059
Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [49:05<08:17, 15.07s/it]epoch: 167 validateAccuracy: 0.2804 trainAccuracy: 1.0 delta: -0.001
Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [49:20<08:02, 15.08s/it]epoch: 168 validateAccuracy: 0.2824 trainAccuracy: 1.0 delta: 0.001
Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [49:37<08:06, 15.71s/it]epoch: 169 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [49:52<07:45, 15.52s/it]epoch: 170 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [50:07<07:26, 15.38s/it]epoch: 171 validateAccuracy: 0.2794 trainAccuracy: 1.0 delta: -0.003
Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [50:22<07:08, 15.31s/it]epoch: 172 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [50:37<06:51, 15.24s/it]epoch: 173 validateAccuracy: 0.2804 trainAccuracy: 1.0 delta: -0.002
Training:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [50:52<06:34, 15.19s/it]epoch: 174 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [51:07<06:18, 15.13s/it]epoch: 175 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.0089
Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [51:23<06:03, 15.14s/it]epoch: 176 validateAccuracy: 0.2814 trainAccuracy: 1.0 delta: -0.001
Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [51:38<05:48, 15.16s/it]epoch: 177 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.0089
Training:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [51:53<05:33, 15.15s/it]epoch: 178 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0069
Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [52:08<05:18, 15.15s/it]epoch: 179 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0069
Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [52:23<05:01, 15.10s/it]epoch: 180 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: -0.0079
Training:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [52:38<04:46, 15.09s/it]epoch: 181 validateAccuracy: 0.2716 trainAccuracy: 1.0 delta: -0.0108
Training:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [52:53<04:30, 15.05s/it]epoch: 182 validateAccuracy: 0.2735 trainAccuracy: 1.0 delta: -0.0089
Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [53:08<04:15, 15.01s/it]epoch: 183 validateAccuracy: 0.2804 trainAccuracy: 1.0 delta: -0.002
Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [53:23<03:59, 14.99s/it]epoch: 184 validateAccuracy: 0.2725 trainAccuracy: 1.0 delta: -0.0099
Training:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [53:38<03:45, 15.01s/it]epoch: 185 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0069
Training:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [53:53<03:30, 15.01s/it]epoch: 186 validateAccuracy: 0.2725 trainAccuracy: 1.0 delta: -0.0099
Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [54:08<03:14, 14.99s/it]epoch: 187 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: -0.0079
Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [54:23<03:00, 15.01s/it]epoch: 188 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.004
Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [54:38<02:45, 15.00s/it]epoch: 189 validateAccuracy: 0.2765 trainAccuracy: 1.0 delta: -0.0059
Training:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [54:53<02:29, 14.99s/it]epoch: 190 validateAccuracy: 0.2745 trainAccuracy: 1.0 delta: -0.0079
Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [55:08<02:15, 15.00s/it]epoch: 191 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.004
Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [55:23<02:00, 15.07s/it]epoch: 192 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.004
Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [55:38<01:45, 15.05s/it]epoch: 193 validateAccuracy: 0.2755 trainAccuracy: 1.0 delta: -0.0069
Training:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [55:53<01:30, 15.07s/it]epoch: 194 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.004
Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [56:08<01:15, 15.06s/it]epoch: 195 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [56:23<01:00, 15.05s/it]epoch: 196 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [56:38<00:45, 15.08s/it]epoch: 197 validateAccuracy: 0.2775 trainAccuracy: 1.0 delta: -0.0049
Training:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [56:54<00:30, 15.09s/it]epoch: 198 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.004
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [57:09<00:15, 15.11s/it]epoch: 199 validateAccuracy: 0.2784 trainAccuracy: 1.0 delta: -0.004
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [57:24<00:00, 15.11s/it]Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [57:24<00:00, 17.22s/it]
Loading data...
[38;5;2m[i 0509 18:38:27.323171 84 dataset.py:631] Found 102 classes and 1020 images.[m
[38;5;2m[i 0509 18:38:27.351637 84 dataset.py:631] Found 102 classes and 1020 images.[m
[38;5;2m[i 0509 18:38:27.404951 84 dataset.py:631] Found 102 classes and 6149 images.[m
Traceback (most recent call last):
  File "main.py", line 285, in <module>
    train(model=mlpMixerModel,modelName="MLPMixer",learningRate=2.3e-5,epochs=200,etaMin=1e-5,imageSize=224,savedName="1_1.pkl")
  File "main.py", line 271, in train
    testAccuracy,testLoss=get_test_data_set_accuracy(model,imageSize)
  File "main.py", line 199, in get_test_data_set_accuracy
    return validate_one_epoch(model,testDataSet)
TypeError: validate_one_epoch() missing 1 required positional argument: 'criterion'
